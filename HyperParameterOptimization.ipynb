{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a-2_3SXMMvD"
      },
      "outputs": [],
      "source": [
        "# Create a smaller dataset (e.g., 10% of the original)\n",
        "sample_fraction=0.1\n",
        "df_sampled = df.sample(frac=sample_fraction, random_state=42)  # Use a random state for reproducibility\n",
        "\n",
        "# Update X and y with the sampled data\n",
        "X_sampled = df_sampled.drop(columns=['Class'])\n",
        "y_sampled = df_sampled['Class']\n",
        "\n",
        "# ... (Rest of your code using X_sampled and y_sampled for initial experiments) ...\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define hyperparameter search spaces for each model\n",
        "param_dists = {\n",
        "    \"Logistic Regression\": {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']  # Choose appropriate solvers\n",
        "    },\n",
        "    \"Decision Tree\": {\n",
        "        'max_depth': [3, 5, 7, 10, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    \"Naive Bayes\": {\n",
        "        # Usually has few hyperparameters to tune\n",
        "        'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    },\n",
        "    \"Neural Network\": {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'alpha': [0.0001, 0.001, 0.01]\n",
        "    },\n",
        "    \"Adaboost\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 1.0],\n",
        "        'algorithm': ['SAMME', 'SAMME.R']\n",
        "    },\n",
        "    \"Random Forest\": {  # Already defined earlier\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, 15, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Optimizing hyperparameters for {model_name}\")\n",
        "\n",
        "    # Get the hyperparameter search space for the current model\n",
        "    param_dist = param_dists[model_name]\n",
        "\n",
        "    if model_name in [\"Logistic Regression\", \"Decision Tree\", \"Naive Bayes\", \"Adaboost\"]:\n",
        "        # Use GridSearchCV with param_dist instead of param_grid\n",
        "        grid_search = GridSearchCV(model, param_grid=param_dist, cv=skf, scoring='f1', n_jobs=-1)\n",
        "        grid_search.fit(X_sampled, y_sampled)\n",
        "        best_params = grid_search.best_params_\n",
        "    else:\n",
        "        # Use RandomizedSearchCV\n",
        "        random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, cv=skf, scoring='f1', n_jobs=-1, random_state=42)\n",
        "        random_search.fit(X_sampled, y_sampled)\n",
        "        best_params = random_search.best_params_\n",
        "\n",
        "    print(f\"  Best hyperparameters: {best_params}\")\n"
      ]
    }
  ]
}